{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mitsouma/Deep-Learning_Labs/blob/main/introduction_practical_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNkV91UBAax8"
      },
      "source": [
        "#### pip install --upgrade tensorflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q738WBQqAax-"
      },
      "source": [
        "Following the release of deep learning libraries, higher-level API-like libraries came out, which sit on top of the deep learning libraries, like TensorFlow, which make building, testing, and tweaking models even more simple. One such library that has easily become the most popular is Keras.\n",
        "\n",
        "Keras has become so popular, that it is now a superset, included with TensorFlow releases now! If you're familiar with Keras previously, you can still use it, but now you can use tensorflow.keras to call it. By that same token, if you find example code that uses Keras, you can use with the TensorFlow version of Keras too. In fact, you can just do something like:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGlQWbRSAax3"
      },
      "source": [
        "The mathematical challenge for the artificial neural network is to best optimize thousands or millions or whatever number of weights you have, so that your output layer results in what you were hoping for. Solving for this problem, and building out the layers of our neural network model is exactly what TensorFlow is for. TensorFlow is used for all things \"operations on tensors.\" A tensor in this case is nothing fancy. It's a multi-dimensional array.\n",
        "\n",
        "To install TensorFlow, simply do a:\n",
        "\n",
        "\n",
        "\n",
        "###\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "auUCk_P3AayC"
      },
      "outputs": [],
      "source": [
        "import tensorflow.keras as keras\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ggm89mTKAayO"
      },
      "source": [
        "For this tutorial, we are going to be using TensorFlow version 1.10. You can figure out your version:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "JcB1WpmoAayR",
        "outputId": "038eb7b3-be5b-4b2a-b3de-6382c22ecded"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.11.0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "print(tf.__version__)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ST-NgqgvAaya"
      },
      "source": [
        "Once we've got tensorflow imported, we can then begin to prepare our data, model it, and then train it. For the sake of simplicity, we'll be using the most common \"hello world\" example for deep learning, which is the mnist dataset. It's a dataset of hand-written digits, 0 through 9. It's 28x28 images of these hand-written digits. We will show an example of using outside data as well, but, for now, let's load in this data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "r_IurMeZAayc",
        "outputId": "7c1d7c69-2840-4bc3-babf-4b320ae7adb4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "(x_train, y_train),(x_test, y_test) = mnist.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9wHruF4Aayj"
      },
      "source": [
        "What exactly do we have here? Let's take a quick look.\n",
        "\n",
        "\n",
        "So the x_train data is the \"features.\" In this case, the features are pixel values of the 28x28 images of these digits 0-9. The y_train is the label (is it a 0,1,2,3,4,5,6,7,8 or a 9?)\n",
        "\n",
        "The testing variants of these variables is the \"out of sample\" examples that we will use. These are examples from our data that we're going to set aside, reserving them for testing the model.\n",
        "\n",
        "Neural networks are exceptionally good at fitting to data, so much so that they will commonly over-fit the data. Our real hope is that the neural network doesn't just memorize our data and that it instead \"generalizes\" and learns the actual problem and patterns associated with it.\n",
        "\n",
        "Let's look at this actual data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 990
        },
        "id": "GvOtwXEwAayl",
        "outputId": "46ca4b9e-8581-4659-c8fe-0d67348c15dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   3  18  18  18 126 136\n",
            "  175  26 166 255 247 127   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0  30  36  94 154 170 253 253 253 253 253\n",
            "  225 172 253 242 195  64   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0  49 238 253 253 253 253 253 253 253 253 251\n",
            "   93  82  82  56  39   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0  18 219 253 253 253 253 253 198 182 247 241\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0  80 156 107 253 253 205  11   0  43 154\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0  14   1 154 253  90   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0 139 253 190   2   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0  11 190 253  70   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0  35 241 225 160 108   1\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0  81 240 253 253 119\n",
            "   25   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  45 186 253 253\n",
            "  150  27   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  16  93 252\n",
            "  253 187   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 249\n",
            "  253 249  64   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  46 130 183 253\n",
            "  253 207   2   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0  39 148 229 253 253 253\n",
            "  250 182   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0  24 114 221 253 253 253 253 201\n",
            "   78   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0  23  66 213 253 253 253 253 198  81   2\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0  18 171 219 253 253 253 253 195  80   9   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0  55 172 226 253 253 253 253 244 133  11   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0 136 253 253 253 212 135 132  16   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]]\n"
          ]
        }
      ],
      "source": [
        "print(x_train[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_Xo424xAayt"
      },
      "source": [
        "Alright, could we visualize this?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "pBHG2r-1Aayv",
        "outputId": "fe7aac3a-7f79-4aa4-9bfa-ca542172dbac"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f04abaf74e0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "#normal charts inside notebooks\n",
        "%matplotlib inline\n",
        "\n",
        "plt.imshow(x_train[0],cmap=plt.cm.binary)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oannUR18Aay4"
      },
      "source": [
        "How about the value for y_train with the same index?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "cBMDXhGCAay8",
        "outputId": "a0f603f6-d094-413a-ac89-a5f6159ddd95"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5\n"
          ]
        }
      ],
      "source": [
        "print(y_train[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5Zn6SNTAazD"
      },
      "source": [
        "It's generally a good idea to \"normalize\" your data. This typically involves scaling the data to be between 0 and 1, or maybe -1 and positive 1. In our case, each \"pixel\" is a feature, and each feature currently ranges from 0 to 255. Not quite 0 to 1. Let's change that with a handy utility function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GH_b1Wa_AazG"
      },
      "outputs": [],
      "source": [
        "x_train = tf.keras.utils.normalize(x_train, axis=1)\n",
        "x_test = tf.keras.utils.normalize(x_test, axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2448
        },
        "id": "N07pWAhmAazR",
        "outputId": "7bcb49d0-67c0-4f1e-aa4e-0e506f06b643"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.00393124 0.02332955 0.02620568 0.02625207 0.17420356 0.17566281\n",
            "  0.28629534 0.05664824 0.51877786 0.71632322 0.77892406 0.89301644\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.05780486 0.06524513 0.16128198 0.22713296\n",
            "  0.22277047 0.32790981 0.36833534 0.3689874  0.34978968 0.32678448\n",
            "  0.368094   0.3747499  0.79066747 0.67980478 0.61494005 0.45002403\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.12250613 0.45858525 0.45852825 0.43408872 0.37314701\n",
            "  0.33153488 0.32790981 0.36833534 0.3689874  0.34978968 0.32420121\n",
            "  0.15214552 0.17865984 0.25626376 0.1573102  0.12298801 0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.04500225 0.4219755  0.45852825 0.43408872 0.37314701\n",
            "  0.33153488 0.32790981 0.28826244 0.26543758 0.34149427 0.31128482\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.1541463  0.28272888 0.18358693 0.37314701\n",
            "  0.33153488 0.26569767 0.01601458 0.         0.05945042 0.19891229\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.0253731  0.00171577 0.22713296\n",
            "  0.33153488 0.11664776 0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.20500962\n",
            "  0.33153488 0.24625638 0.00291174 0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.01622378\n",
            "  0.24897876 0.32790981 0.10191096 0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.04586451 0.31235677 0.32757096 0.23335172 0.14931733 0.00129164\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.10498298 0.34940902 0.3689874  0.34978968 0.15370495\n",
            "  0.04089933 0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.06551419 0.27127137 0.34978968 0.32678448\n",
            "  0.245396   0.05882702 0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.02333517 0.12857881 0.32549285\n",
            "  0.41390126 0.40743158 0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.32161793\n",
            "  0.41390126 0.54251585 0.20001074 0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.06697006 0.18959827 0.25300993 0.32678448\n",
            "  0.41390126 0.45100715 0.00625034 0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.05110617 0.19182076 0.33339444 0.3689874  0.34978968 0.32678448\n",
            "  0.40899334 0.39653769 0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.04117838 0.16813739\n",
            "  0.28960162 0.32790981 0.36833534 0.3689874  0.34978968 0.25961929\n",
            "  0.12760592 0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.04431706 0.11961607 0.36545809 0.37314701\n",
            "  0.33153488 0.32790981 0.36833534 0.28877275 0.111988   0.00258328\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.05298497 0.42752138 0.4219755  0.45852825 0.43408872 0.37314701\n",
            "  0.33153488 0.25273681 0.11646967 0.01312603 0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.37491383 0.56222061\n",
            "  0.66525569 0.63253163 0.48748768 0.45852825 0.43408872 0.359873\n",
            "  0.17428513 0.01425695 0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.92705966 0.82698729\n",
            "  0.74473314 0.63253163 0.4084877  0.24466922 0.22648107 0.02359823\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]]\n"
          ]
        }
      ],
      "source": [
        "print(x_train[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "0Usg6wXZAazd",
        "outputId": "fcacd6a2-be0e-477e-ccb3-b8965f14f9ab"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f047be45f98>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.imshow(x_train[0],cmap=plt.cm.binary)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChdFN0_7Aazs"
      },
      "source": [
        "# Now let's build our model!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kZ_NlkNVAazw"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.models.Sequential()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzrIrq0eAaz5"
      },
      "source": [
        "A sequential model is what you're going to use most of the time. It just means things are going to go in direct order. A feed forward model. No going backwards...for now.\n",
        "\n",
        "Now, we'll pop in layers. Recall our neural network image? Was the input layer flat, or was it multi-dimensional? It was flat. So, we need to take this 28x28 image, and make it a flat 1x784. There are many ways for us to do this, but keras has a Flatten layer built just for us, so we'll use that."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xtkS5Y8BAaz6"
      },
      "outputs": [],
      "source": [
        "model.add(tf.keras.layers.Flatten())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQKbhqSaAa0B"
      },
      "source": [
        "This will serve as our input layer. It's going to take the data we throw at it, and just flatten it for us. Next, we want our hidden layers. We're going to go with the simplest neural network layer, which is just a Dense layer. This refers to the fact that it's a densely-connected layer, meaning it's \"fully connected,\" where each node connects to each prior and subsequent node. Just like our image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XPFYBqm7Aa0E"
      },
      "outputs": [],
      "source": [
        "model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhQ3RuMNAa0N"
      },
      "source": [
        "This layer has 128 units. The activation function is relu, short for rectified linear. Currently, relu is the activation function you should just default to. There are many more to test for sure, but, if you don't know what to use, use relu to start.\n",
        "\n",
        "Let's add another identical layer for good measure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AMj_Z5BtAa0O"
      },
      "outputs": [],
      "source": [
        "model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtJx1aLRAa0Y"
      },
      "source": [
        "Now, we're ready for an output layer:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8aIw3weyAa0b"
      },
      "outputs": [],
      "source": [
        "model.add(tf.keras.layers.Dense(10, activation=tf.nn.softmax))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JlkteTlAa0k"
      },
      "source": [
        "This is our final layer. It has 10 nodes. 1 node per possible number prediction. In this case, our activation function is a softmax function, since we're really actually looking for something more like a probability distribution of which of the possible prediction options this thing we're passing features through of is. Great, our model is done.\n",
        "\n",
        "Now we need to \"compile\" the model. This is where we pass the settings for actually optimizing/training the model we've defined."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P9IrZIfOAa0l"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNymFaK6Aa0x"
      },
      "source": [
        "Remember why we picked relu as an activation function? Same thing is true for the Adam optimizer. It's just a great default to start with.\n",
        "\n",
        "Next, we have our loss metric. Loss is a calculation of error. A neural network doesn't actually attempt to maximize accuracy. It attempts to minimize loss. Again, there are many choices, but some form of categorical crossentropy is a good start for a classification task like this.\n",
        "\n",
        "Now, we fit!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "AcqbPOkLAa0z",
        "outputId": "6daf183e-8b09-4a82-e34a-7e677b67a675"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "60000/60000 [==============================] - 7s 124us/step - loss: 0.2624 - acc: 0.9241\n",
            "Epoch 2/3\n",
            "60000/60000 [==============================] - 7s 115us/step - loss: 0.1092 - acc: 0.9666\n",
            "Epoch 3/3\n",
            "60000/60000 [==============================] - 7s 114us/step - loss: 0.0735 - acc: 0.9767\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f0478818cc0>"
            ]
          },
          "execution_count": 16,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(x_train, y_train, epochs=3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOz7I2M_Aa0_"
      },
      "source": [
        "As we train, we can see loss goes down (yay), and accuracy improves quite quickly to 98-99% (double yay!)\n",
        "\n",
        "Now that's loss and accuracy for in-sample data. Getting a high accuracy and low loss might mean your model learned how to classify digits in general (it generalized)...or it simply memorized every single example you showed it (it overfit). This is why we need to test on out-of-sample data (data we didn't use to train the model).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "p8pmztovAa1C",
        "outputId": "4290d7c8-e083-4f1a-fbe4-0a77405b7505"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 0s 33us/step\n",
            "0.09391475263349712\n",
            "0.9713\n"
          ]
        }
      ],
      "source": [
        "val_loss, val_acc = model.evaluate(x_test, y_test)\n",
        "print(val_loss)\n",
        "print(val_acc)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "An1AAz-6Aa1R"
      },
      "source": [
        "It's going to be very likely your accuracy out of sample is a bit worse, same with loss. In fact, it should be a red flag if it's identical, or better.\n",
        "\n",
        "finally, make predictions!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sft5qV_7Aa1T"
      },
      "outputs": [],
      "source": [
        "predictions = model.predict(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "id": "vTfTY8RfAa1c",
        "outputId": "55461214-94bc-4a95-c472-bb5275fde5f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[1.1531731e-09 1.6657758e-08 4.3342610e-07 ... 9.9997044e-01\n",
            "  7.1998265e-08 1.4766479e-07]\n",
            " [4.2890966e-10 6.9435482e-05 9.9988151e-01 ... 1.0250092e-08\n",
            "  5.0797122e-09 2.4275922e-11]\n",
            " [3.9731293e-07 9.9911433e-01 5.0606963e-05 ... 4.6153678e-04\n",
            "  2.6475886e-05 7.1926718e-07]\n",
            " ...\n",
            " [5.9835883e-09 1.7595082e-06 6.2936479e-08 ... 1.3065385e-04\n",
            "  5.4760716e-05 6.7713583e-04]\n",
            " [5.9752274e-06 9.0162603e-06 1.2241945e-07 ... 3.1561780e-05\n",
            "  1.2025710e-03 4.3655952e-08]\n",
            " [7.0423226e-08 5.0259255e-09 6.1181844e-08 ... 2.3204232e-10\n",
            "  1.8224696e-08 4.8948740e-10]]\n"
          ]
        }
      ],
      "source": [
        "print(predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eg1uSqcdAa1j"
      },
      "source": [
        "That sure doesn't start off as helpful, but recall these are probability distributions. We can get the actual number pretty simply:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "cfL6IAQlAa1k",
        "outputId": "34646b60-a064-4a21-dbdf-471654462d58"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "7\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "print(np.argmax(predictions[0]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7kB_zvKAa1v"
      },
      "source": [
        "There's your prediction, let's look at the input:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "rydoGCmXAa1w",
        "outputId": "5a18a2ec-7784-491e-dfec-1f9ea645ada8"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f0478449780>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.imshow(x_test[0],cmap=plt.cm.binary)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6r8-erJAa19"
      },
      "source": [
        "Awesome! Okay, I think that covers all of the \"quick start\" types of things with Keras. This is just barely scratching the surface of what's available to you, so start poking around Tensorflow and Keras documentation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kCzeIvfpAa2A"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}